{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.9.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Youtube Data Scraping using YouTube Data API v3\n",
    "The following Jupyter Notebook allows one to get YouTube video metadata and statistics from a YouTube channel. Using, Official `YouTube Data API v3` by Google, this notebook allows you get video title, description, URL, published datetime as well as video statistics - likes, dislikes, comment count and views for almost all videos in a channel. The `YouTube Data API v3` allows access for the recent 20,000 of a channel. There is no straightforward way to access earlier videos from a channel. In this notebook, along with access to the recent 20,000 videos, I also provide functions to access older videos through a combination of `playlist` and `search` attribute.         \n",
    "**Edited By: Purushottam Mohanty**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from googleapiclient.discovery import build # google api\n",
    "import pandas as pd # data cleaning\n",
    "import datetime # handling datetime objects\n",
    "from dateutil import tz # handling timezones\n",
    "\n",
    "# setup api builds\n",
    "youTubeApiKey = 'YOUR_YOUTUBE_DATA_API_V3_KEY' \n",
    "youtube = build('youtube','v3',developerKey=youTubeApiKey)"
   ]
  },
  {
   "source": [
    "### Get Channel ID\n",
    "The first part of the code allows one to get channel ID for legacy Youtube Channel whose username is visible instead of channel ID on the URL."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get channel id using username for a legacy youtube channel\n",
    "channel = youtube.channels().list(part=\"id\", forUsername=\"ndtv\").execute()\n",
    "channelId = channel['items'][0]['id']\n",
    "channelId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set channelID \n",
    "channelId = 'UCef1-8eOpJgud7szVPlZQAQ' # CNN - News 18"
   ]
  },
  {
   "source": [
    "### Youtube Channel Statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting channel snippet data\n",
    "snippetdata = youtube.channels().list(part='snippet',id=channelId).execute()\n",
    "snippetdata\n",
    "\n",
    "# getting channel statistics\n",
    "statdata = youtube.channels().list(part='statistics',id=channelId).execute()\n",
    "stats = statdata['items'][0]['statistics']\n",
    "stats"
   ]
  },
  {
   "source": [
    "## Part 1\n",
    "## Data for the Recent 20000 Videos\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all video details\n",
    "contentdata = youtube.channels().list(id=channelId,part='contentDetails').execute()\n",
    "playlist_id = contentdata['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "videos = [ ]\n",
    "next_page_token = None\n",
    "\n",
    "while 1:\n",
    "    res = youtube.playlistItems().list(playlistId=playlist_id,part='snippet',maxResults=50,pageToken=next_page_token).execute()\n",
    "    videos += res['items']\n",
    "    next_page_token = res.get('nextPageToken')\n",
    "    if next_page_token is None:\n",
    "        break\n",
    "print(len(videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting video id for each video\n",
    "video_ids = list(map(lambda x:x['snippet']['resourceId']['videoId'], videos))\n",
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting statistics for each video\n",
    "stats = []\n",
    "for i in range(0, len(video_ids), 40):\n",
    "    res = (youtube).videos().list(id=','.join(video_ids[i:i+40]),part='statistics').execute()\n",
    "    stats += res['items']\n",
    "print(len(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all information in a list\n",
    "videoid = []\n",
    "title = []\n",
    "channeltitle = []\n",
    "publishdate = []\n",
    "likes = []\n",
    "dislikes = []\n",
    "views = [ ]\n",
    "url = [ ]\n",
    "commentcount = [ ]\n",
    "description = []\n",
    "\n",
    "for i in range(len(videos)):\n",
    "      videoid.append(videos[i]['snippet']['resourceId']['videoId'])\n",
    "      title.append(videos[i]['snippet']['title'])\n",
    "      channeltitle.append(videos[i]['snippet']['videoOwnerChannelTitle'])\n",
    "      publishdate.append(videos[i]['snippet']['publishedAt'])\n",
    "      url.append(\"https://www.youtube.com/watch?v=\" + videos[i]['snippet']['resourceId']['videoId'])\n",
    "      likes.append(int(stats[i]['statistics'].get('viewCount', \"0\")))\n",
    "      dislikes.append(int(stats[i]['statistics'].get('dislikeCount', \"0\")))\n",
    "      views.append(int(stats[i]['statistics'].get('viewCount', \"0\")))\n",
    "      commentcount.append(int(stats[i]['statistics'].get('commentCount', \"0\")))\n",
    "      description.append(videos[i]['snippet'].get('description', \"NA\"))\n",
    "\n",
    "# saving lists together as a dataframe\n",
    "df = pd.DataFrame({'videoid':videoid, 'title':title, 'channeltitle':channeltitle, 'publishdate':publishdate, 'url':url, 'likes':likes, 'dislikes':dislikes, 'views':views, 'commentcount':commentcount, 'description':description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data \n",
    "path = \"SET_DATA_OUTPUT_PATH.csv\"\n",
    "df.to_csv(path_or_buf=path, sep=\",\", header=True)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 2\n",
    "## Get All Playlists from Channel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all playlists belonging to the channel\n",
    "playlists = []\n",
    "next_page_token = None\n",
    "\n",
    "while 1:\n",
    "    res = youtube.playlists().list(channelId=channelId,part='snippet',maxResults=50,pageToken=next_page_token).execute()\n",
    "    playlists += res['items']\n",
    "    next_page_token = res.get('nextPageToken')\n",
    "    if next_page_token is None:\n",
    "        break\n",
    "# total number of playlists\n",
    "print(len(playlists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Playlists Before/After a Particular Date\n",
    "# Get all Playlist IDs and Publish Date\n",
    "playlist_ids = []\n",
    "playlist_date = []\n",
    "\n",
    "for i in range(len(playlists)):\n",
    "    playlist_ids.append(playlists[i]['id'])\n",
    "    playlist_date.append(playlists[i]['snippet']['publishedAt'])\n",
    "\n",
    "# Make Data Frame\n",
    "df_playlists = pd.DataFrame({'playlist_id':playlist_ids, 'playlist_date':playlist_date})\n",
    "# convert column to datetime object\n",
    "df_playlists['playlist_date'] = pd.to_datetime(df_playlists['playlist_date'], format='%Y-%m-%dT%H:%M:%SZ').dt.tz_localize(tz.tzlocal())\n",
    "df_playlists = df_playlists.sort_values(by='playlist_date', ascending=False)\n",
    "df_playlists.reset_index(inplace=True)\n",
    "\n",
    "# select playlists within a time period\n",
    "df_playlists_selected = df_playlists[(df_playlists['playlist_date'] >= '2019-01-01') & (df_playlists['playlist_date'] < '2019-12-31')]\n",
    "df_playlists_selected.reset_index(inplace=True)\n",
    "# total number of playlists\n",
    "print(len(df_playlists_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all videos within each playlist\n",
    "videos = [ ]\n",
    "\n",
    "for i in range(len(df_playlists_selected)):\n",
    "    next_page_token = None\n",
    "    playlist_id = df_playlists_selected['playlist_id'][i]\n",
    "    # get all videos within the playlist\n",
    "    while 1:\n",
    "        res = youtube.playlistItems().list(playlistId=playlist_id,part='snippet',maxResults=50,pageToken=next_page_token).execute()\n",
    "        videos += res['items']\n",
    "        next_page_token = res.get('nextPageToken')\n",
    "        if next_page_token is None:\n",
    "            break\n",
    "print(len(videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting video ids for each video\n",
    "video_ids = list(map(lambda x:x['snippet']['resourceId']['videoId'], videos))\n",
    "print(len(video_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting statistics for each video\n",
    "stats = []\n",
    "for i in range(0, len(video_ids), 40):\n",
    "    res = (youtube).videos().list(id=','.join(video_ids[i:i+40]),part='statistics').execute()\n",
    "    stats += res['items']\n",
    "print(len(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos and stats needs to be merged as some video ids have been made private\n",
    "# dataframe of video ids\n",
    "videoid1 = []\n",
    "title = []\n",
    "channeltitle = []\n",
    "publishdate = []\n",
    "url = []\n",
    "description = []\n",
    "\n",
    "for i in range(len(videos)):\n",
    "    videoid1.append(videos[i]['snippet']['resourceId']['videoId'])\n",
    "    title.append(videos[i]['snippet']['title'])\n",
    "    channeltitle.append(videos[i]['snippet'].get('videoOwnerChannelTitle'))\n",
    "    publishdate.append(videos[i]['snippet']['publishedAt'])\n",
    "    url.append(\"https://www.youtube.com/watch?v=\" + videos[i]['snippet']['resourceId']['videoId'])\n",
    "    description.append(videos[i]['snippet'].get('description', \"NA\"))\n",
    "\n",
    "df_videos = pd.DataFrame({'videoid':videoid1, 'title':title, 'channeltitle':channeltitle, 'publishdate':publishdate, 'url':url, 'description':description})\n",
    "\n",
    "# dataframe of statistics\n",
    "videoid2 = []\n",
    "likes = []\n",
    "dislikes = []\n",
    "views = []\n",
    "commentcount = []\n",
    "\n",
    "for i in range(len(stats)):\n",
    "    videoid2.append(stats[i].get('id'))\n",
    "    likes.append(int(stats[i]['statistics'].get('viewCount', \"0\")))\n",
    "    dislikes.append(int(stats[i]['statistics'].get('dislikeCount', \"0\")))\n",
    "    views.append(int(stats[i]['statistics'].get('viewCount', \"0\")))\n",
    "    commentcount.append(int(stats[i]['statistics'].get('commentCount', \"0\")))\n",
    "\n",
    "df_stats = pd.DataFrame({'videoid':videoid2, 'likes':likes, 'dislikes':dislikes, 'views':views, 'commentcount':commentcount})\n",
    "\n",
    "# merge two dataframes and keep interesection of the two\n",
    "df = df_videos.merge(df_stats, how='inner', on='videoid')\n",
    "df = df.drop_duplicates()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to existing data\n",
    "df_old = pd.read_csv(\"/Users/purushottam/Dropbox (Personal)/research/media_bias/data/yt_timesnow.csv\")\n",
    "# append datasets\n",
    "df_final = pd.concat([df_old,df])\n",
    "# sort by date (recent first)\n",
    "df_final = df_final.sort_values(by=['publishdate'],ascending=False)\n",
    "df_final = df_final.drop_duplicates()\n",
    "# order columns\n",
    "cols = ['channeltitle','videoid','publishdate','title','url','description','likes','dislikes','views','commentcount']\n",
    "df_final = df_final[cols]\n",
    "len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data\n",
    "export_path = 'SET_OUTPUT_PATH.csv'\n",
    "df_final.to_csv(path_or_buf=export_path, sep=\",\", header=True, index=False)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Extract Metadata For Pre-20000 Videos\n",
    "Videos older than 20000 videos can also be extracted using the `search` attribute of the `YouTube Data API v3` but attribute suffers from bugs which han't been fixed as of `25 May 2021`. While the attribute does work, `search` returns only an arbitary subset of videos posted by the channel. Additionally, `search` attribute is expensive and costs `100` daily quotas. However, the attribute does have parameters such as `publishedAfter` and `publishedBefore` which can be used to return results for a specific time period."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Video IDs for videos pre-20000 videos\n",
    "# Note Search has a quota of 100\n",
    "publishedAfter  = datetime.datetime(2019, 7, 1, 0, 0, 0).astimezone().isoformat()\n",
    "publishedBefore = datetime.datetime(2020, 3, 5, 0, 0, 0).astimezone().isoformat()\n",
    "videos = [ ]\n",
    "next_page_token = None\n",
    "\n",
    "while 1:\n",
    "    searchdata = youtube.search().list(part='id,snippet',channelId=channelId,publishedAfter=publishedAfter, publishedBefore=publishedBefore,order='date',maxResults=50,pageToken=next_page_token).execute()\n",
    "    videos += searchdata['items']\n",
    "    next_page_token = searchdata.get('nextPageToken')\n",
    "    if next_page_token is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Video IDs for pre-20000 Videos for Specified Time Period \n",
    "video_ids = []\n",
    "for i in range(len(videos)):\n",
    "    video_ids.append(videos[i]['id'].get('videoId', \"NA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index of NAs from video_ids list\n",
    "na_ids = [i for i, x in enumerate(video_ids) if x == 'NA']\n",
    "# remove those values from video list\n",
    "videos = [i for j, i in enumerate(videos) if j not in na_ids]\n",
    "# remove those values from video_ids list\n",
    "video_ids = [i for j, i in enumerate(video_ids) if j not in na_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Metadata for pre-20000 Videos for Specified Time Period \n",
    "stats = []\n",
    "for i in range(0, len(video_ids), 40):\n",
    "    res = (youtube).videos().list(id=','.join(video_ids[i:i+40]),part='statistics').execute()\n",
    "    stats += res['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all information in a list\n",
    "videoid = []\n",
    "title = []\n",
    "channeltitle = []\n",
    "publishdate = []\n",
    "likes = []\n",
    "dislikes = []\n",
    "views = [ ]\n",
    "url = [ ]\n",
    "commentcount = [ ]\n",
    "description = []\n",
    "\n",
    "for i in range(len(videos)):\n",
    "      videoid.append(videos[i]['id'].get('videoId', \"NA\"))\n",
    "      title.append(videos[i]['snippet']['title'])\n",
    "      channeltitle.append(videos[i]['snippet']['channelTitle'])\n",
    "      publishdate.append(videos[i]['snippet']['publishedAt'])\n",
    "      url.append(\"https://www.youtube.com/watch?v=\" + videos[i]['id'].get('videoId', \"NA\"))\n",
    "      likes.append(int(stats[i]['statistics'].get('viewCount', \"0\")))\n",
    "      dislikes.append(int(stats[i]['statistics'].get('dislikeCount', \"0\")))\n",
    "      views.append(int(stats[i]['statistics'].get('viewCount', \"0\")))\n",
    "      commentcount.append(int(stats[i]['statistics'].get('commentCount', \"0\")))\n",
    "      description.append(videos[i]['snippet'].get('description', \"NA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all list to data frames\n",
    "df_pre20000 = pd.DataFrame({'videoid':videoid, 'title':title, 'channeltitle':channeltitle, 'publishdate':publishdate, 'url':url, 'likes':likes, 'dislikes':dislikes, 'views':views, 'commentcount':commentcount, 'description':description})\n",
    "len(df_pre20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to existing data\n",
    "old_data_path = 'OLD_DATA_PATH.csv'\n",
    "df_old = pd.read_csv(old_data_path)\n",
    "# append datasets\n",
    "df_final = pd.concat([df_old,df_pre20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframe\n",
    "export_path = 'SET_OUTPUT_PATH.csv'\n",
    "df_final.to_csv(path_or_buf=export_path, sep=\",\", header=True)"
   ]
  }
 ]
}